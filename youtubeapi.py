# -*- coding: utf-8 -*-
"""YoutubeAPI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1093noYAJvflpyKNZZiTfokoEnOA9fHyq
"""

# IMPORTS FOR THE WEB APP
!pip install anvil-uplink

#connecting the anvil server locally
import anvil.server
anvil.server.connect("server_FJWHW4UA2Z4IG4CPMSTYLPQE-AHELUHS3FZ3L5QXJ")

"""# Youtube Content Blocker Text Classification

## Members
*   Hana Sheto
*   Mary Wilkinson
*   Me Crouch

## Things to Consider to Improve

### If we were turning in right now, we'd probably pass... the code works.

But it probably wouldn't be a high-mark cause we haven't done much actual AI work. This is where the project technically begins. There are many ways to improve it... here are the first 4 I can think of, not all may help... This part is getting out of my "I'm pretty sure I know what I'm doing" zone. ~ MeC.


1.   **Better Data: Clean code better or differently**

2.   **More Data**

      Just more video playlists, maybe more of a specific type, or a more diverse set of videos.

3.   **Changing, Adding, Removing Layers and Layer types**
      
      This has a lot of options, but they are more "figure it out" by looking at many diverse examples and looking at tensorflow documentation.     

4.   **Changing the variables and settings**
    
      Several options to change, I have tried to throw them in one code block above the Tensorflow code.

5.   **???**

      Any other ideas?

# Code

The code below, SHOULD be able to be autoran. It will take some time to download 700+ videos worth of string data. And it will toss out a third of that as not usable and it will erase 20% of that 2/3rds that is usable as non-alpha or unwanted punctuation. And out of all that, we get our clean data.

Not all is needed though. I will try to make note when it is **NOT** needed to run.

#### Reset Data. If this is your first run, push to set variables.
"""

all_videos = []
all_videos_data = []
all_labels = []
RAW_all_videos_data = []

# These are videos on the ban (mostly, if not all) playlist that I culled due one reason or another. ~ Me
# Some of them were grabbed probably due to title or thumbnail glance. Some culled because while the visual data may be blockworthy, the comments or captions may not make it seem so and thus muddy the data.
skipList = ['8LhkyyCvUHk', 'i1FGWQt1kkA', '-gg6JhlrdxA', 'uEbZK3YJAY8', 'stGLHIdwtnY', 'EDjaiK0G-5A', 's6X-1ShM8uA', 'yfLPYRcmLbQ', '5Ddap2Pyhtw', 'zzT413KKy-s', 'f-JVkvbtxls', 'X8f--Ljgrh4', 'wV1FrqwZyKw', 'lh5lv23SShY', '4wwtqqgb6-o']

"""## Imports and Setting up the Environment

### Imports
"""

# Insure API is installed into Environment
!pip install youtube_transcript_api

# Import necessary libraries
import youtube_transcript_api
import googleapiclient.discovery as discovery
import googleapiclient.errors
import re
import string
import json
from IPython.display import clear_output # Autoclearing of Colab outputs, not used in code anymore, but VERY useful in some testing cases. Keeping in for revisiting later.
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import losses
from sklearn.model_selection import train_test_split  # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html

"""### Variables for Gathering Data"""

# This is a Google "ProjectID" API, specific to our HW assignment... anyone (of us) can use this, can ONLY access public data.
# By only looking at public data, it removes a lot of headaches in coding.
API_KEY = 'AIzaSyAwP04IwBdriwO6CXtDuLK20UdINqBSTBA' # Me's Personal
#API_KEY = 'AIzaSyAOQk9mszD74jtfnahpenD5ApKH9eFs7I8'   # Me's Truman
#API_KEY = 'AIzaSyAvf6K8yyZzpkMhcRsK17g4eDWZbNj36pU' # Hana's

MAX_COMMENT_PAGES = 2 # See paragraph below

# Setting up the API object that talks with Youtube
DEVELOPER_KEY = API_KEY
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'
youtube = discovery.build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)

file_path = 'data.txt'

"""I first had MAX_PAGES on 10, it takes a long time, and the final accuracy of Validation didn't seem to improve, but getting two pages of comments, does give us more data per string. But, the top comments will be the ones that provide most insight into the video, so the more you get, the more likely you are to get unimportant data.

### Custom Functions (Quicker to minimize this section and play all.)

Not all of the custom functions are here currently... But these are the main ones before getting into model compiling.
"""

# Most of these are from the top 100 words used in English. Some MAY be useful to keep in,
# but without running an algorithm to count all words in both output classes, and comparing,
# we can't really tell which ones.
removeList = [
    "the", "and", "you", "that", "was", "for", "are", "with",
    "his", "they","this", "have", "from", "one", "had", "word", "but", "not", "what",
    "all", "were", "when", "your", "can", "said", "there", "use", "each", "which", "she", "how",
    "their", "will", "other", "about", "out", "many", "then", "them", "these", "some", "her",
    "would", "make", "like", "him", "into", "time", "has", "look", "two", "more", "write", "see", "number",
    "way", "could", "people", "than", "first", "water", "been", "call", "who", "oil", "its", "now",
    "find","did", "get", "come", "made", "may", "part", "just",
]

# Removing common contractions to get more specific
contractions_dict = {"won't": "will not",
                     "can't": "can not",
                     "n't": " not",
                     "'re": " are",
                     "'s": " is",
                     "'d": " would",
                     "'ll": " will",
                     "'t": " not",
                     "'ve": " have",
                     "'m": " am",}

# pre-compile regex patterns
contractions_pattern = re.compile("|".join(contractions_dict.keys()))
DIGIT_PATTERN = re.compile(r"\d")
WORD_PATTERN_1 = re.compile(r"\b_([a-zA-z]+)_\b")
WORD_PATTERN_2 = re.compile(r"\b_([a-zA-z]+)\b")
WORD_PATTERN_3 = re.compile(r"\b([a-zA-z]+)_\b")

def clean_text(f):
    # Strip, Lowercase, and Remove Non-ASCII Characters
    f = f.lower()
    f = f.encode('ascii', 'ignore').decode('ascii')

    # remove brackets
    f = re.sub(r"\[[^\]]*\]", "", f)
    f = re.sub(r"\([^\)]*\)", "", f)
    f = re.sub(r"<.*?>", "", f)

    # replace contractions
    f = contractions_pattern.sub(lambda match: contractions_dict[match.group(0)], f)

    # remove unwanted characters using str.translate()
    f = f.translate(str.maketrans("", "", '\n\t-\\/,"\''))

    # keep only alphabets
    f = re.sub(r"[^a-zA-Z]", " ", f)

    # remove words <2 or >15
    f = re.sub(r"\b\w{1,2}\b|\b\w{15,}\b", "", f)

    # remove words from removeList
    for word in removeList:
        f = re.sub(fr"\b{word}\b", "", f)

    # remove extra spaces
    f = re.sub(r" {2,}", " ", f)
    f = " ".join(f.split())
    return f

def clean_all_data(RAW_all_videos_data):
  return_list = []
  for each in RAW_all_videos_data:
    return_list.append(clean_text(each))

  return return_list

def vectorize_text(text, label):
  '''
    Vectorize text data for the TensorFlow transformer.

    Args:
        text (str): Text data to be vectorized.
        label (int): Label for the text data.

    Returns:
        tuple: A tuple consisting of vectorized text data and its label.
    '''
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label

def custom_standardization(input_data):
  '''
    Preprocess text input to clean up text data for the TensorFlow model.

    Args:
        input_data (str): Text data to be preprocessed.

    Returns:
        str: A cleaned version of the input text data.
  '''
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  stripped_html = tf.strings.regex_replace(stripped_html,r'\d+(?:\.\d*)?(?:[eE][+-]?\d+)?', ' ')
  stripped_html = tf.strings.regex_replace(stripped_html, r'@([A-Za-z0-9_]+)', ' ' )
  stripped_html = tf.strings.regex_replace(stripped_html, 'http\S+', ' ')
  stripped_newLine = tf.strings.regex_replace(stripped_html, '\n', ' ')

  return tf.strings.regex_replace(stripped_newLine, '[%s]' % re.escape(string.punctuation), '')

def grabTopLevelComments(response_page):
  '''
    Retrieve top-level comments for a specific response page of comments.

    Args:
        response_page (list): A list of comment threads from a response page.

    Returns:
        str: A concatenated string of all the top-level comments in the response page.
    '''
  text = ''
  for item in response_page:
    text = text + ' ' + item['snippet']['topLevelComment']['snippet']['textDisplay']
  return text

def grab_captions(videoID):
  '''
    Retrieves the captions of a YouTube video with the specified video ID.

    Args:
    videoID (str): The unique video ID of the YouTube video for which captions are to be retrieved.

    Returns:
    str: A string containing the concatenated captions of the video.
  '''
  caption_string = ''

  try:
    transcript_list = youtube_transcript_api.YouTubeTranscriptApi.list_transcripts(videoID)
    transcript = transcript_list.find_transcript(['en'])
    print(f"ID: {transcript.video_id} \n Video Lanage: {transcript.language}/{transcript.language_code} \n Generated: {transcript.is_generated} \n Translatable:{transcript.is_translatable} \n")

    captions = transcript.fetch()

    for segment in captions:
      caption_string = caption_string + " " + segment['text']
  except youtube_transcript_api.CouldNotRetrieveTranscript as e:
    print(e.CAUSE_MESSAGE, e.CAUSE_MESSAGE_INTRO)

  return caption_string

def grab_comments(videoID, yt, vidNums):
    comment_str = ''
    try:
      request = yt.commentThreads().list(
          part="snippet,replies",
          textFormat="plainText",
          videoId=videoID
          )
      print(f"Video {vidNums[0]}:{vidNums[1]} | Grabbing comments for videoID:{videoID} Page 1")
      response = request.execute()

      comment_str = grabTopLevelComments(response['items'])

      i = 1 # We've already grabbed the first page of comments.
      while ('nextPageToken' in response) and (i < MAX_COMMENT_PAGES):
        print(f"Video {vidNums[0]}:{vidNums[1]} | Grabbing comments for videoID:{videoID} Page {i}")

        nextPageToken = response['nextPageToken']
        response = yt.commentThreads().list(
            part="snippet,replies",
            textFormat="plainText",
            videoId=videoID,
            pageToken=nextPageToken
            ).execute()
        i += 1
        comment_str = comment_str + " " + grabTopLevelComments(response['items'])
    except googleapiclient.errors.HttpError as e:
      # print(f"HttpError:{e.error_details[0]['reason']}, ignoring video.")
      # A warning is already printed by Colab.
      pass

    return comment_str

def get_video_data(videoID, yt, vidNums):
  '''
    Get caption and comment data for a specific video.

    Args:
        videoID (str): The ID of the video to retrieve data from.
        yt (googleapiclient.discovery.Resource): A Google API client object used to retrieve comments.
        vidNums (tuple): A tuple containing the current video and maximum video integers to update user on progress.

    Returns:
        str: A concatenated string of all the captions and comments for the video.
    '''
  print("---------------------------------------")
  print(f"Video {vidNums[0]}:{vidNums[1]} | Grabbing captions for videoID:{videoID}")

  # Retrieve Captions
  cap_str = grab_captions(videoID)
  #print(cap_str)

  # Retrieve Comments
  com_str = grab_comments(videoID, yt, vidNums)
  #print(com_str)

  return cap_str + " " + com_str

# Really useful in understanding some of the string statistics, in order to
# better clean them.
def printAnalytics(list_of_strs):
  max_string_len = 0
  min_string_len = 1000           # We for sure have smaller and larger strings, and this will drop below this.
  total_length_of_strings = 0

  for str in list_of_strs:
    str_length = len(str)
    if str_length < min_string_len:
      min_string_len = str_length

    if str_length > max_string_len:
      max_string_len = str_length

    total_length_of_strings += str_length

  avg_string_len = total_length_of_strings/len(list_of_strs)

  print(f'''
      Max Length: {max_string_len}
      Min Length: {min_string_len}
      Avg Length: {avg_string_len}
      ''')
  return max_string_len, min_string_len, avg_string_len

"""## Open Data from File

Pointless to run if you don't have the data.txt, but useful if you do. Save below to acquire.

If this fails, it's because you are in a new session. Run everything else below in order and skip this.
"""

with open(file_path, 'r') as file:
    data = json.load(file)

all_videos = data['all_videos']
all_labels = data['all_labels']
all_videos_data = data['all_videos_data']
RAW_all_videos_data = data['RAW_all_videos_data']
skipList = data['skipList']

"""## Find New Videos (Insert Playlists Here)

Adapted and built off Hana's original code, using API instead of HTTP Requests

Add playlists to sus_ or good_ playlists, they will sort themselves out.

If this is a new runtime, the data will need to be downloaded, but it will save the data (if you are autorunning or you play all codeblocks below) so that way a consistant runtime will never download duplicate data.
"""

# https://youtube.com/playlist?list=PL-1iM8r5Y-09K7LsjObJLoIz8N9CKScxY (Everything after list= in the URL)
# Don't place an empty string in. It doesn't like that.

sus_playlists = [
    'PLGm0g9kYgt2EyeKoUbNGns3MmxM0JvcoI',
    'PLSg9uHxhv5-75lAQdtYVtsP7IjJsJCLS8',
    'PLD6llDYTAHUm1Hl-XmKz1O0a06lZDlI7y',
    'PL0XXWlWERlGkoI0CMqUrZSDa1EVruuWfh',
    'PLGm0g9kYgt2Ehkmb3l8bYBV_KAKHxmLtM',
    'PLGm0g9kYgt2G4AIp_aB_W4A8vVruYUKbW',
    'PLCzvjB3WERyyEGYTSE9YqKsZs0FZ_KK6u',
    ]
good_playlists = [
    'PLGO_AWB1C4GQz6JF3-0yZHpoKfqZb7O5z',
    'PLGO_AWB1C4GS3Kb5RZtdJGBemD7HTTfC_',
    'PLGO_AWB1C4GTSm249JRyVxYLByaNrpVQt',
    'PLOklt6-bKH9jb2jinz8I1IVbGrwZM9auq',
    'PLhz12vamHOnYmvLSYtQvuxDrWSi795yDa',
    'PLhz12vamHOnaY7nvpgtQ0SIbuJdC4HA5O',
    'PLhz12vamHOnZ4ZDC0dS6C9HRN5Qrm0jHO',
    'PLhz12vamHOnZv8kM6Xo6AbluwIIVpulio',
    'PLMeD3sfITvelEa8JYNuOUSsbQVFgrptJg',
    'PLlbkyhAZrBl-XJQudaCfoMsGy_Jjau6HE',
    'PL9bvS3PRAOb2CGw_w-dQ0G6_ioIHIecCs',
    'PLFwhUkU59Z5vFpB4mMNhj8vvu05b60VnK',
    'PLcpk5TCg3vzfFoNIsmxY92jQjTZN7Vlw3',
    'PLyrgGY-qeL4RCk37hGQjw5zUbNN2TJZHA',
    'PLfRZ-LcYAZo39e9-f-dlMwbZx-vjhPfT9',
    ]

all_playlists = sus_playlists + good_playlists
new_videos = []

# Loop looks at each playlist and appends all videoIDs to all_videos list.
for playlist in all_playlists:
  request = youtube.playlistItems().list(part="id,snippet", playlistId=playlist, maxResults=50) # Has a max of 50.
  response = request.execute()

  # Specifically grabs the videoID and appends to all_videos list and appends the appropriate label to label list.
  for video in response['items']:
    videoID = video['snippet']['resourceId']['videoId']
    if (videoID not in skipList) and (videoID not in all_videos):
      new_videos.append(videoID)
      if playlist in sus_playlists:
        all_labels.append(1)
      else:
        all_labels.append(0)

"""### Print Videos, Labels, and New Video Lists (Not needed to run, but shows data)

Print Above Gathered Variables for Verification
Useful in knowing if you need to download data, the number of videos total and in each class, and confirming that all lists are of same length (if there are new videos to download, labels will be longer by the length of new_videos list.)
"""

print(f"Length of all_videos_data: {len(all_videos_data)}")
print(f"Length of RAW_all_videos_data: {len(RAW_all_videos_data)}")
print(f"Length of all_videos: {len(all_videos)}")
print(all_videos)
print(f"Length of all_labels: {len(all_labels)}")
print(all_labels)
print(f"Length of new_videos: {len(new_videos)}")
print(new_videos)

x = 0
for i in all_labels:
  if i == 1:
    x += 1

print(f"Total: {len(all_labels)} | Blocked:Safe {x}:{len(all_labels) - x}")

"""## Grab Videos Comments, Captions, etc.
Currently a full clean run takes just under 10 minutes.

Only run this more than once per session if you added/removed videos or playlists, as regular running without saving and using that data will make you cap on our Quota. IF it refuses to download more because of Quota, change to a new API up in Variables for Gathering Data section.
"""

nVids = len(new_videos)
i = 1

for video in new_videos:
  RAW_all_videos_data.append(get_video_data(video, youtube, (i, nVids)))
  i += 1

all_videos = all_videos + new_videos

"""## Save Downloaded Information

Hana, your suggestion was VERY useful. This only saves to the specific runtime environment however, meaning it still needs to run once per 'session' but sometimes it can reconnect if you come back to it. This does save a LOT of extra calls while adjusting code. (Run reset at top of code and then download to reset.)

The data.txt file can be downloaded and uploaded in future sessions to skip downloading.
"""

# Clean data before saving.
all_videos_data = clean_all_data(RAW_all_videos_data)

lists_to_write = ['all_videos', 'all_labels', 'all_videos_data', 'RAW_all_videos_data', 'skipList']
data = {listname: globals()[listname] for listname in lists_to_write}

with open(file_path, 'w') as file:
    json.dump(data, file, indent=2)

"""## View Data

### Print Analytics of All the Videos' String Data
(This... IS required in a full clean run. Because I decided to make one while loop do two seperate things... and ... yeah. This isn't the cleanest code in this colab I'll admit.)

Technically this also checks string lengths, and adds to skip list... We REALLY should break this into two seperate steps. But it works...
"""

vid = 0

while vid < len(all_videos_data):
  if len(all_videos_data[vid]) < 1000 or len(all_videos_data[vid]) > 10000:
    if all_videos[vid] not in skipList:
      skipList.append(all_videos[vid])
      print(f"Added {all_videos[vid]} to skipList")
  vid += 1

max_string_len, min_string_len, avg_string_len = printAnalytics(all_videos_data)

# Remove Skipped Videos From all Data Lists (Resave Data after Running)
for each in skipList:
  i = None
  try:
    i = all_videos.index(each)
  except ValueError:
    pass  # I know, I'm quitely passing the error... but the baby is asleep and I don't want to wake em.
  if i != None:
    all_videos.pop(i)
    all_labels.pop(i)
    all_videos_data.pop(i)
    RAW_all_videos_data.pop(i)

print(len(all_videos))

"""### Print Specific Video Information
(Don't Run Unless you want to search for a specific video by ID or index.)
"""

# To See any specific video in list.
vid = all_videos.index('')        # Add string, or comment out and assign the index you wish to see to vid.
print(f"Video {vid} of {len(all_labels)} | ID: {all_videos[vid]}")
print(f"Label: {all_labels[vid]} (0=SAFE, 1=BLOCK)")
print(f"String: {all_videos_data[vid]}")
print(f"Raw String: {RAW_all_videos_data[vid]}")

"""## Building Tensorflow Datasets from Raw Data

### Variables, Settings
"""

batch_size = 4           # Default: 32    4 works best for our datasets at least. Bigger batches are probably better with more data?
seed = 42                #                Seed never really changed, Did test some with using None as seed for random. Kept in for consistancy in our current phase.
max_features = 5000      # Default: 10000 5000 was the sweet spot thus far. With more data, a larger max would probably be okay.
sequence_length = 512    # Default: 250

validation_size = 0.2    # Default: 0.1   Validation size... I've played with some... but with smaller dataset, we con't have too much wiggle room.

# Variables for Models (Put in to ensure that model and export model have same settings if changed.)
OPTIMIZER = 'adam'       # adam was best... never changed. If we had images, I think one of the other optimizers would have been good.
METRICS = tf.metrics.BinaryAccuracy(threshold=0.0)
LOSS = losses.BinaryCrossentropy(from_logits=True)

DROPOUT_1 = 0.1
DROPOUT_2 = 0.1

embedding_dim = 128      # I think 64 was the default, we can go bigger here and it mildly may help... but train time increases, and doesn't help enough to worry.

"""### Split and Create Raw Data into Datasets"""

# Using the sklearn.model_selection.train_test_split function to split up the data and labels into approapriate sets.
# Link on the function is in the import section.
texts_train, texts_val, labels_train, labels_val = train_test_split(all_videos_data, all_labels, test_size=validation_size, random_state=seed, stratify=all_labels)

# Convert the lists to TensorFlow datasets
raw_full_ds = tf.data.Dataset.from_tensor_slices((all_videos_data, all_labels))
raw_train_ds = tf.data.Dataset.from_tensor_slices((texts_train, labels_train))
raw_val_ds = tf.data.Dataset.from_tensor_slices((texts_val, labels_val))

# Shuffle and batch the datasets
raw_full_ds = raw_full_ds.shuffle(buffer_size=len(all_videos_data)).batch(batch_size=batch_size)
raw_train_ds = raw_train_ds.shuffle(buffer_size=len(texts_train)).batch(batch_size=batch_size)
raw_val_ds = raw_val_ds.batch(batch_size=batch_size)

"""## Transformer Code

This tutorial helps explain the vectorizing and mapping of text for input layers. [Word Embeddings Tutorial](https://www.tensorflow.org/text/guide/word_embeddings)

May be helpful in explaining a step or two of the below for us.

### Vectorizing, Tokenizing, and Standardizing
"""

## Standardize, Tokenize, and Vectorize

# Creates the vect-layer from tf library.
vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization, # Custom function above
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)

### Creates text only dataset for training purposes.
# Specifically, this finds the [max_features] most common words.
# This is also equal to the input shape of the model - 1
train_text = raw_full_ds.map(lambda x, y: x)

# Must call adapt to not leak information.
# It adapts the vectorized layer to the training text...
# But I don't know specifically what this does.
vectorize_layer.adapt(train_text)

# Final Preprocessing Step: Mapping the data to the vectorized_text
train_ds = raw_train_ds.map(vectorize_text) # Custom function above
val_ds = raw_val_ds.map(vectorize_text)

# Configuring Dataset for Performance
# (.cache() and .prefetch() prevent blocking (when one thread can't access data blocked by another thread, and both are held blocked.))
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# Not Required to Run
# Ideally this would give us the batch_size of data and training labels (TO-DO, labeling data according to source playlist? on our data.)
text_batch, label_batch = next(iter(raw_train_ds))
first_video, first_label = text_batch[0], label_batch[0] # <--- If we wanted to look at the batch for any reason.
# These tokens are integers that can be looked at what they equate to with .get_vocabulary() on the layer.
print(" 0 ---> ",vectorize_layer.get_vocabulary()[0])
print(" 1 ---> ",vectorize_layer.get_vocabulary()[1])
print(" 2 ---> ",vectorize_layer.get_vocabulary()[2])
print(" 3 ---> ",vectorize_layer.get_vocabulary()[3])
print(" 4 ---> ",vectorize_layer.get_vocabulary()[4])
print(" 5 ---> ",vectorize_layer.get_vocabulary()[5])
print(" 6 ---> ",vectorize_layer.get_vocabulary()[6])
print(" 7 ---> ",vectorize_layer.get_vocabulary()[7])
print(" 8 ---> ",vectorize_layer.get_vocabulary()[8])
print(" 9 ---> ",vectorize_layer.get_vocabulary()[9])
print(" 10 ---> ",vectorize_layer.get_vocabulary()[10])
print(" 100 ---> ",vectorize_layer.get_vocabulary()[100])
print(" 1000 ---> ",vectorize_layer.get_vocabulary()[1000])
print(" final ---> ",vectorize_layer.get_vocabulary()[-1])
print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))

"""### Building and Compiling Models

If Autoran, default Model will run. Choose specific model's build function and then run training to change. Copy and delete the layers of the default model to build your own.

*   Convolutional Layers (CONV)
  *   Pulls out features
*   Activation Layers (ACT or RELU)
  *   Usually hidden inside of other layers within Tensorflow because they are required, but can be set up as their own layer as well.
  *   Where we use the same or the actual activation function
*   Pooling (POOL)
  *   Pooling reduces size.
*   Fully connected (FC)
  *   Dense is an example
*   Batch normalization (BN)
  *   ... Never tested
*   Dropout (DO)
  *   Dropout reduces size.

Below is a collection of sets of attempts. The default model is what we started with. The build_compile_model version of default has always been the best, with the data that we have. It's not too dissimiliar to what would be used on the IMDB dataset examples seen in class. Which makes sense as both are binary text classification with rudimentary sentiment analysis problems. Most if not all of our variables changed, and I did try with different added in layers, but they never came out with as good of end-results as the IMDB model. The LSTM model probably would be better in the long run, but it barely trains in colab due to minimal free resources. But the rudimentary version below will run, it just takes way too long to train, considering how well the other does in a fraction of the time.

**The bottom model is the best to specifically run. Need to run at least one of the build_compile_model functions**

---
PS from Me

And as I am cleaning up the file... I realized that when I set this part here up after the original code, I programmed it in a functional programming style not object-oriented. And I'm not sure why, as all of my prior experience is in OOP (outside of Javascripts fake functional oop) Maybe I've been doing too much javascript lately, maybe colab feels more functional style to Me... idk. But it's an interesting thought for me to ponder.
"""

# Creating the default Model
def build_default_model(show_summary=True):
  epochs = 1

  model = tf.keras.Sequential([
    layers.Embedding(max_features + 1, embedding_dim),
    layers.Dropout(DROPOUT_1),
    layers.GlobalAveragePooling1D(),
    layers.Dropout(DROPOUT_2),
    layers.Dense(1)])

  model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)

  if show_summary == True:
    model.summary()

  return model, epochs

# Creates an LSTM
# Still not training well

def build_compile_model(show_summary=True):
  epochs = 7 # Definitely don't need the default 15
  LOSS = losses.BinaryCrossentropy(from_logits=False)

  model = tf.keras.models.Sequential()
  model.add(layers.Embedding(max_features + 1, embedding_dim))
  model.add(layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)))
  model.add(layers.Dropout(.2))
  model.add(layers.Bidirectional(tf.keras.layers.LSTM(32)))
  model.add(layers.Dropout(.2))
  model.add(layers.Dense(1, activation='sigmoid'))

  model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)

  if show_summary == True:
    model.summary()

  # Reset Values Changed to Default
  LOSS = losses.BinaryCrossentropy(from_logits=True)
  return model, epochs

# Creating a RNN Model
# Still not training well

def build_compile_model(show_summary=True):
  epochs = 5

  METRICS = tf.metrics.BinaryAccuracy(threshold=0.0)
  LOSS = losses.BinaryCrossentropy(from_logits=True)
  DROPOUT_1 = 0.2
  DROPOUT_2 = 0.2

  model = tf.keras.Sequential()
  model.add(layers.Embedding(
      max_features,
      output_dim=64,
      # Use masking to handle the variable sequence lengths
      mask_zero=True)),
  model.add(layers.Dropout(DROPOUT_1))
  model.add(layers.Bidirectional(tf.keras.layers.LSTM(64)))
  model.add(layers.Dropout(DROPOUT_2))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(1))

  model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)

  if show_summary == True:
    model.summary()

  # Reset Values Changed to Default
  LOSS = losses.BinaryCrossentropy(from_logits=True)
  METRICS = tf.metrics.BinaryAccuracy(threshold=0.0)
  DROPOUT_1 = 0.1
  DROPOUT_2 = 0.1
  return model, epochs

# Creating the default Model
def build_compile_model(show_summary=True):
  epochs = 4  # Default was 10

  METRICS = tf.metrics.BinaryAccuracy(threshold=0.0)
  LOSS = losses.BinaryCrossentropy(from_logits=True)

  DROPOUT_1 = 0.5
  DROPOUT_2 = 0.25

  embedding_dim = 128

  model = tf.keras.Sequential()
  model.add(layers.Embedding(max_features+1, embedding_dim))
  model.add(layers.GlobalAveragePooling1D()) # Performs downsampling by outputting the average of the time or spatial dimensions of the input.
  model.add(layers.Dense(128))
  model.add(layers.Dropout(DROPOUT_1))
  model.add(layers.Dense(64))
  model.add(layers.Dropout(DROPOUT_2))
  model.add(layers.Dense(32))
  model.add(layers.Dropout(DROPOUT_2))
  model.add(layers.Dense(16))
  model.add(layers.Dense(1))

  model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)

  if show_summary == True:
    model.summary()

  METRICS = tf.metrics.BinaryAccuracy(threshold=0.0)
  LOSS = losses.BinaryCrossentropy(from_logits=True)

  DROPOUT_1 = 0.2
  DROPOUT_2 = 0.2

  embedding_dim = 128

  return model, epochs

"""### Training"""

model, epochs = build_compile_model()
print(f"Running Compiled Model, {epochs} epochs")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)

"""### Evaluating"""

# The naive IMDB dataset achieves an initial accuracy of about 86% for benchmarking purposes.
# None of our models did that well initially, but we also have smaller data, less batches to train per epoch, etc.

# Creates Plot of Accuracy and Loss over time graphs
# the model.fit returns the history object for this.
history_dict = history.history
history_dict.keys()

# -----
acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

figure, axis = plt.subplots(1, 2)

axis[0].plot(epochs, loss, 'bo', label='Training loss')
axis[0].plot(epochs, val_loss, 'b', label='Validation loss')
axis[0].set_title("Loss Over Time")

axis[1].plot(epochs, acc, 'bo', label='Training acc')
axis[1].plot(epochs, val_acc, 'b', label='Validation acc')
axis[1].set_title("Acc Over Time")
plt.show()

"""### Exporting
If we want to export the model by adding in the vectorizing layer in so we can process raw strings...
This part will need more looking at for our specific stuff maybe?
"""

export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS
)

"""Translating the below output... anything above a .9 that I have tried, I wouldn't let my 5 year old watch... with that said, a Satisfactory game development vlog, scores a 98%... and while it's too technical for a child to understand, isn't specifically graphic or explicit as far as I know (I haven't watch it, but this guy is a professional and it's an official company's channel.)

Currently our safe data may be weighted towards child-friendly, and that video is over-the-head of a child. Child friendly shows, score much lower than 90%, so... technically, even with our current dataset I think it is safe to use for younger children... I wouldn't offer it publically yet, but I would feel safe using it on a family member with an adult still within listening range to confirm that a child isn't getting on something too sensitive.
"""

export_test = [     # LowScore < .7 | MidScore is between .7 and .9 | HighScore > .9
  #'xtlTR6oqWR0',   # LowScore Blue's Clues Video, Aimed at children
  #'plaAN46-7U0',   # LowScore D&D Playtest Review, Aimed at teens/adults, technical but not offensive, explicit, or graphic
  #'oR_RAp73ra0',   # HighScore Sabine Hossenfelder, mostly a physicist or something, but talking on transgender and the sciences behind it, and looking at it if it's a social fad. (Fairly neutral on viewpoint, but highly scientific)
  #'9o_Z61p9SNk',   # MidScore 8 True Scary Stories to keep you up at night,
  #'iPORevGYrBI',   # MidScore Jack the Whipper, Can be raunchy replaces 'whip' related words in popular songs, But this one is somewhat PG-13
  #'1ESIqx6_rrE',   # MidScore Kink vs. BDSM: What's the Difference... Adult Topics (Midscore may be because it is plainly spoken, and we may not have had too much sex talk in our training data)
  #'Z4pkE3OFpkc',   # HighScore Randy Feltface - Very raunchy comedian, not for kids.
  #'vZ24w0L5-_M',   # LowScore (.64, so not very low) Scooby-Doo! | Scooby and Shaggy On The Run! Kid Friendly (Could very well share words with some of our scary block material because of ghost/haunting related vocabulary)
  #'j725e4ARiv8',   # LowScore I Built a Minecraft House using Architectural Engineering Theory
  #'J6oXpwKuXQk',   # VERY HighScore SNL Weekend Update MOCKS Putin Attacks U.S for "SATNISM" and "MANY GENDERS"
  #'26bqQttDRaU',   # HighScore Biden to Run for 2nd Term, Trump "Respects and Admires" Melania: Late Night's News of the Week
  #'6_2hzRopPbQ',   # VERY LowScore, [2.035299e-05] Tensorflow in 10 minutes
  #'Pg13IKAAcNw',   # HighScore, Have I Got a Bit More News for You S63 E6. Richard Ayoade... This may not be explicit, but it's british comedy about the news, and there is cursing and whatnot...
]

for video in export_test:
  data_str = get_video_data(video, youtube, (1, 1))

export_model.predict([data_str])

# I can save.. no problem. The warning pops up... and the internet doesn't seem to think it's an issue.
# However, I can't run predict successfully on any model that I save and then upload.

export_model.save('saved_model')

"""# Resources
---
The below dataset of videos are all
* Each video must be public and have at least 1000 views
* Each video must be between 120 and 500 seconds long
* Each video must be associated with at least one entity from our target vocabulary
* Adult & sensitive content is removed (as determined by automated classifiers)
Which means that we could use these as a verified safe dataset. As it's the Adult and sensitive content we want removed, and this guarentees it. But we would still need to find more sensitive data, which I have a small list of some specific to "creepypasta" style videos that I searched for the other night.
Also any playlist needs to contain no more than 50 videos, we will just need "x" amount of playlists.

https://research.google.com/youtube8m/

---
Specific channels to find videos likely to be blocked

https://famisafe.wondershare.com/parental-control/most-blocked-youtube-channels.html

---
https://monkeylearn.com/blog/text-classification-machine-learning/

---
#### Specific Links for Datasets and Splitting

https://www.tensorflow.org/datasets/overview

https://www.tensorflow.org/api_docs/python/tf/data/Dataset

https://www.tensorflow.org/datasets/splits

#### Links to Examples of preprocessing Data

https://www.tensorflow.org/tfx/tutorials/transform/census

---
https://scikit-learn.org/stable/model_selection.html

---
This tutorial introduced text classification from scratch. To learn more about the text classification workflow in general, check out the [Text classification guide](https://developers.google.com/machine-learning/guides/text-classification/) from Google Developers.

---

# Files to upload and test other datasets

Replace file name with dataset of clean string data.

(CSV should contain a column for strings, and a column for labels. Labels need to be ints-- currently this is set up to assign 'good' and 'bad' into ints.)
"""

# This was a trial, huge... train time. even at a smaller list. But I have an idea of what to look for in validation now. (This is sentiment analysis of tweets related to ChatGPT.)
import csv

file = open('file.csv')
csvreader = csv.reader(file)
header = []
header = next(csvreader)
rows = []
for row in csvreader:
        rows.append(row)
file.close()

all_tweets = []
all_tweet_labels = []

for each in rows:
  if each[2] == 'neutral':
    pass
  else:
    all_tweets.append(each[1])
    if each[2] == 'good':
      all_tweet_labels.append(0)
    else:
      all_tweet_labels.append(1)
all_tweets = clean_all_data(all_tweets)          # I even tried limiting to just 20,000, and our current model would take a long time to train. LSTM would go for hours or longer.
texts_train, texts_val, labels_train, labels_val = train_test_split(all_tweets[0:20000], all_tweet_labels[0:20000], test_size=validation_size, random_state=seed, stratify=all_tweet_labels[0:20000])

# Convert the lists to TensorFlow datasets
raw_full_ds = tf.data.Dataset.from_tensor_slices((all_tweets, all_tweet_labels))
raw_train_ds = tf.data.Dataset.from_tensor_slices((texts_train, labels_train))
raw_val_ds = tf.data.Dataset.from_tensor_slices((texts_val, labels_val))

# Shuffle and batch the datasets
raw_full_ds = raw_full_ds.shuffle(buffer_size=len(all_videos_data)).batch(batch_size=batch_size)
raw_train_ds = raw_train_ds.shuffle(buffer_size=len(texts_train)).batch(batch_size=batch_size)
raw_val_ds = raw_val_ds.batch(batch_size=batch_size)

"""# Anvil Web App connection"""

@anvil.server.callable
def predict_video(videoID):
  data = get_video_data(videoID, youtube, (1, 1))
  data = clean_text(data)
## Using the Model

  if export_model.predict([data])[0][0] > .9:
    return True
  else:
    return False

anvil.server.wait_forever()

predict_video('xtlTR6oqWR0')